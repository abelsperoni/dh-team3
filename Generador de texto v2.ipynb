{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "#Largo de las frases\n",
    "train_df['phrase_len'] = train_df.text.str.split().str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitamos el dataset a frases de 30 palabras para simplificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = train_df[train_df['phrase_len'] <= 30].reset_index()\n",
    "max_phrase_len = phrases['phrase_len'].max()\n",
    "max_phrase_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando un tokenizer, codificamos los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a 17693-word vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters= '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "tokenizer.fit_on_texts(phrases['text'])\n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "print(f'We have a {vocab_len}-word vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos padding para que sean de la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phrases['sequences'] = tokenizer.texts_to_sequences(phrases['text'])\n",
    "sequences = phrases['sequences'].values\n",
    "sequences_padded = pad_sequences(phrases['sequences'], maxlen=max_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13337, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- probamos esto y no funcionó ---\n",
    "Agregamos mas muestras desplazando las frases hacia la derecha.\n",
    "El objetivo del modelo va a ser predecir la ultima palabra de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# upsampled_sequences = list()\n",
    "# #upsampled_sequences.append(sequences_padded)\n",
    "# for i in range(sequences_padded.shape[0]):\n",
    "#     upsampled_sequences.append(sequences_padded[i])\n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end='\\r')\n",
    "#     sequence = sequences_padded[i]\n",
    "#     for _ in range(len(sequences[i])-3):\n",
    "#         sequence = np.append(0,sequence[:-1])\n",
    "#         upsampled_sequences.append(sequence)\n",
    "# upsampled_sequences = np.array(upsampled_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampled_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la matriz de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Se encontraron 16338 vectores de embedding, sobre un total de 17693 palabras. \n",
      "        1355 palabras van a ser representadas con todos ceros\n"
     ]
    }
   ],
   "source": [
    "glove_file = './glove.6b/glove.6B.100d.txt'\n",
    "glove_dim = 100\n",
    "vocab = tokenizer.word_index.keys()\n",
    "embeddings_index = {}\n",
    "f = open(glove_file, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'''\n",
    "        Se encontraron {len(embeddings_index)} vectores de embedding, sobre un total de {vocab_len} palabras. \n",
    "        {vocab_len - len(embeddings_index)} palabras van a ser representadas con todos ceros''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relacionamos glove con el tokenizer definido anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_len, glove_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar, armamos un modelo con una capa embedding usando glove.\n",
    "La salida del modelo van a ser las frases con sus vectores de embeddings asociados\n",
    "Definimos el target como la ultima palabra de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        embedded shape: (13337, 30, 100). \n",
      "        13337 frases, \n",
      "        30 palabras,\n",
      "        100 dimensiones de embedding\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "embedding_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_len,\n",
    "                            glove_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_phrase_len,\n",
    "                            trainable=False)\n",
    "embedding_model.add(embedding_layer)\n",
    "embedding_model.compile('rmsprop', 'mse')\n",
    "\n",
    "embedded = embedding_model.predict(sequences_padded)\n",
    "print(f'''\n",
    "        embedded shape: {embedded.shape}. \n",
    "        {embedded.shape[0]} frases, \n",
    "        {embedded.shape[1]} palabras,\n",
    "        {embedded.shape[2]} dimensiones de embedding\n",
    "        ''')\n",
    "x = embedded[:,:-1,:]\n",
    "y = sequences_padded[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un modelo que intente predecir la ultima palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_layer = Input(shape=(x.shape[1:]))\n",
    "generator_LSTM_layer_1 = LSTM(128, return_sequences=True)(generator_input_layer)\n",
    "generator_LSTM_layer_2 = LSTM(256)(generator_LSTM_layer_1)\n",
    "dense_layer_1 = Dense(1024, activation='relu')(generator_LSTM_layer_2)\n",
    "output_layer = Dense(vocab_len, activation='softmax')(dense_layer_1)\n",
    "\n",
    "generator_model = Model(generator_input_layer, output_layer, name=\"generator_model\")\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "generator_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 29, 100)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 29, 128)           117248    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17693)             18135325  \n",
      "=================================================================\n",
      "Total params: 18,909,981\n",
      "Trainable params: 18,909,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13337 samples\n",
      "Epoch 1/40\n",
      "13337/13337 [==============================] - 71s 5ms/sample - loss: 3.0946\n",
      "Epoch 2/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 2.5666\n",
      "Epoch 3/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 2.1440\n",
      "Epoch 4/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 1.7392\n",
      "Epoch 5/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 1.3994\n",
      "Epoch 6/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 1.1597\n",
      "Epoch 7/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.9704\n",
      "Epoch 8/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.7986\n",
      "Epoch 9/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.6859\n",
      "Epoch 10/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 0.6142\n",
      "Epoch 11/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 0.5523\n",
      "Epoch 12/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4808\n",
      "Epoch 13/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4654\n",
      "Epoch 14/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4647\n",
      "Epoch 15/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4165\n",
      "Epoch 16/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4289\n",
      "Epoch 17/40\n",
      "13337/13337 [==============================] - 75s 6ms/sample - loss: 0.3958\n",
      "Epoch 18/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4129\n",
      "Epoch 19/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3874\n",
      "Epoch 20/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.3997\n",
      "Epoch 21/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3932\n",
      "Epoch 22/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4085\n",
      "Epoch 23/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3963\n",
      "Epoch 24/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4022\n",
      "Epoch 25/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3780\n",
      "Epoch 26/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4090\n",
      "Epoch 27/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3864\n",
      "Epoch 28/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4050\n",
      "Epoch 29/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4001\n",
      "Epoch 30/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 0.3989\n",
      "Epoch 31/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3836\n",
      "Epoch 32/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3985\n",
      "Epoch 33/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4014\n",
      "Epoch 34/40\n",
      "13337/13337 [==============================] - 75s 6ms/sample - loss: 0.4184\n",
      "Epoch 35/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3916\n",
      "Epoch 36/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4155\n",
      "Epoch 37/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4023\n",
      "Epoch 38/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4330\n",
      "Epoch 39/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4019\n",
      "Epoch 40/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163071211c8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.save_weights(\"generator_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_json = generator_model.to_json()\n",
    "with open(\"generator_json.json\", \"w\") as json_file:\n",
    "    json_file.write(generator_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator_model.load_weights('generator_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13337, 29, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo generando una frase de 30 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random text: \n",
      "['your acting is very natural as i live']\n",
      "generated text:\n",
      "live tell course it much be rowena so air lived angelic it course child appeared day by annoyance me ruin was her needed companions doré alone him true dust day \n"
     ]
    }
   ],
   "source": [
    "random_sample = np.random.randint(0, x.shape[0] - 1)\n",
    "generated_sequence = sequences_padded[random_sample]\n",
    "print('random text: ')\n",
    "print(f'{tokenizer.sequences_to_texts([generated_sequence,])}')\n",
    "print(f'generated text:')\n",
    "for i in range(30):\n",
    "    generated_embedded = embedding_model.predict(generated_sequence.reshape(1,30))\n",
    "    predicted_word_sequence = generator_model.predict(generated_embedded[:,:-1,:])\n",
    "    #next_index = sample(predicted_word_sequence, 1)\n",
    "    next_index = predicted_word_sequence.argmax()\n",
    "    generated_sequence = np.append(generated_sequence,next_index)[1:]\n",
    "\n",
    "    next_word = tokenizer.sequences_to_texts([[next_index,],])[0]\n",
    "    sys.stdout.write(next_word + ' ')\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
