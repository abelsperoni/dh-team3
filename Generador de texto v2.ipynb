{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "#Largo de las frases\n",
    "train_df['phrase_len'] = train_df.text.str.split().str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitamos el dataset a frases de 30 palabras para simplificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = train_df[train_df['phrase_len'] <= 30].reset_index()\n",
    "max_phrase_len = phrases['phrase_len'].max()\n",
    "max_phrase_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando un tokenizer, codificamos los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a 17693-word vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters= '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'')\n",
    "tokenizer.fit_on_texts(phrases['text'])\n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "print(f'We have a {vocab_len}-word vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos padding para que sean de la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phrases['sequences'] = tokenizer.texts_to_sequences(phrases['text'])\n",
    "sequences = phrases['sequences'].values\n",
    "sequences_padded = pad_sequences(phrases['sequences'], maxlen=max_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13337, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- probamos esto y no funcion√≥ ---\n",
    "Agregamos mas muestras desplazando las frases hacia la derecha.\n",
    "El objetivo del modelo va a ser predecir la ultima palabra de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.85 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# upsampled_sequences = list()\n",
    "# #upsampled_sequences.append(sequences_padded)\n",
    "# for i in range(sequences_padded.shape[0]):\n",
    "#     upsampled_sequences.append(sequences_padded[i])\n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end='\\r')\n",
    "#     sequence = sequences_padded[i]\n",
    "#     for _ in range(len(sequences[i])-3):\n",
    "#         sequence = np.append(0,sequence[:-1])\n",
    "#         upsampled_sequences.append(sequence)\n",
    "# upsampled_sequences = np.array(upsampled_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampled_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la matriz de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Se encontraron 16338 vectores de embedding, sobre un total de 17693 palabras. \n",
      "        1355 palabras van a ser representadas con todos ceros\n"
     ]
    }
   ],
   "source": [
    "glove_file = './glove.6b/glove.6B.100d.txt'\n",
    "glove_dim = 100\n",
    "vocab = tokenizer.word_index.keys()\n",
    "embeddings_index = {}\n",
    "f = open(glove_file, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word not in vocab:\n",
    "        continue\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'''\n",
    "        Se encontraron {len(embeddings_index)} vectores de embedding, sobre un total de {vocab_len} palabras. \n",
    "        {vocab_len - len(embeddings_index)} palabras van a ser representadas con todos ceros''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relacionamos glove con el tokenizer definido anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_len, glove_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar, armamos un modelo con una capa embedding usando glove.\n",
    "La salida del modelo van a ser las frases con sus vectores de embeddings asociados\n",
    "Definimos el target como la ultima palabra de cada frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        embedded shape: (13337, 30, 100). \n",
      "        13337 frases, \n",
      "        30 palabras,\n",
      "        100 dimensiones de embedding\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "embedding_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_len,\n",
    "                            glove_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_phrase_len,\n",
    "                            trainable=False)\n",
    "embedding_model.add(embedding_layer)\n",
    "embedding_model.compile('rmsprop', 'mse')\n",
    "\n",
    "embedded = embedding_model.predict(sequences_padded)\n",
    "print(f'''\n",
    "        embedded shape: {embedded.shape}. \n",
    "        {embedded.shape[0]} frases, \n",
    "        {embedded.shape[1]} palabras,\n",
    "        {embedded.shape[2]} dimensiones de embedding\n",
    "        ''')\n",
    "x = embedded[:,:-1,:]\n",
    "y = sequences_padded[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un modelo que intente predecir la ultima palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_layer = Input(shape=(x.shape[1:]))\n",
    "generator_LSTM_layer_1 = LSTM(128, return_sequences=True)(generator_input_layer)\n",
    "generator_LSTM_layer_2 = LSTM(256)(generator_LSTM_layer_1)\n",
    "dense_layer_1 = Dense(1024, activation='relu')(generator_LSTM_layer_2)\n",
    "output_layer = Dense(vocab_len, activation='softmax')(dense_layer_1)\n",
    "\n",
    "generator_model = Model(generator_input_layer, output_layer, name=\"generator_model\")\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "generator_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 29, 100)]         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 29, 128)           117248    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17693)             18135325  \n",
      "=================================================================\n",
      "Total params: 18,909,981\n",
      "Trainable params: 18,909,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13337 samples\n",
      "Epoch 1/40\n",
      "13337/13337 [==============================] - 71s 5ms/sample - loss: 3.0946\n",
      "Epoch 2/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 2.5666\n",
      "Epoch 3/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 2.1440\n",
      "Epoch 4/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 1.7392\n",
      "Epoch 5/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 1.3994\n",
      "Epoch 6/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 1.1597\n",
      "Epoch 7/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.9704\n",
      "Epoch 8/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.7986\n",
      "Epoch 9/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.6859\n",
      "Epoch 10/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 0.6142\n",
      "Epoch 11/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 0.5523\n",
      "Epoch 12/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4808\n",
      "Epoch 13/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4654\n",
      "Epoch 14/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4647\n",
      "Epoch 15/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4165\n",
      "Epoch 16/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4289\n",
      "Epoch 17/40\n",
      "13337/13337 [==============================] - 75s 6ms/sample - loss: 0.3958\n",
      "Epoch 18/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4129\n",
      "Epoch 19/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3874\n",
      "Epoch 20/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.3997\n",
      "Epoch 21/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3932\n",
      "Epoch 22/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4085\n",
      "Epoch 23/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3963\n",
      "Epoch 24/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4022\n",
      "Epoch 25/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3780\n",
      "Epoch 26/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4090\n",
      "Epoch 27/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3864\n",
      "Epoch 28/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4050\n",
      "Epoch 29/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4001\n",
      "Epoch 30/40\n",
      "13337/13337 [==============================] - 74s 6ms/sample - loss: 0.3989\n",
      "Epoch 31/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3836\n",
      "Epoch 32/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3985\n",
      "Epoch 33/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4014\n",
      "Epoch 34/40\n",
      "13337/13337 [==============================] - 75s 6ms/sample - loss: 0.4184\n",
      "Epoch 35/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.3916\n",
      "Epoch 36/40\n",
      "13337/13337 [==============================] - 72s 5ms/sample - loss: 0.4155\n",
      "Epoch 37/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4023\n",
      "Epoch 38/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4330\n",
      "Epoch 39/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4019\n",
      "Epoch 40/40\n",
      "13337/13337 [==============================] - 73s 5ms/sample - loss: 0.4193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x163071211c8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model.save_weights(\"generator_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_json = generator_model.to_json()\n",
    "with open(\"generator_json.json\", \"w\") as json_file:\n",
    "    json_file.write(generator_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator_model.load_weights('generator_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13337, 29, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo generando una frase de 30 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random text: \n",
      "['your acting is very natural as i live']\n",
      "generated text:\n",
      "live tell course it much be rowena so air lived angelic it course child appeared day by annoyance me ruin was her needed companions dor√© alone him true dust day \n"
     ]
    }
   ],
   "source": [
    "random_sample = np.random.randint(0, x.shape[0] - 1)\n",
    "generated_sequence = sequences_padded[random_sample]\n",
    "print('random text: ')\n",
    "print(f'{tokenizer.sequences_to_texts([generated_sequence,])}')\n",
    "print(f'generated text:')\n",
    "for i in range(30):\n",
    "    generated_embedded = embedding_model.predict(generated_sequence.reshape(1,30))\n",
    "    predicted_word_sequence = generator_model.predict(generated_embedded[:,:-1,:])\n",
    "    #next_index = sample(predicted_word_sequence, 1)\n",
    "    next_index = predicted_word_sequence.argmax()\n",
    "    generated_sequence = np.append(generated_sequence,next_index)[1:]\n",
    "\n",
    "    next_word = tokenizer.sequences_to_texts([[next_index,],])[0]\n",
    "    sys.stdout.write(next_word + ' ')\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
